{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\daniil anisimov\\.conda\\envs\\quist3\\lib\\site-packages (0.13.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement inspect (from versions: none)\n",
      "ERROR: No matching distribution found for inspect\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'inspect_ai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minspect_ai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlog\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_eval_log\n\u001b[0;32m      3\u001b[0m log \u001b[38;5;241m=\u001b[39m read_eval_log(log_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2025-03-26T23-43-33+01-00_nested-dirs-challenge_DxYcXZSDzKP9aKqDKAoYhq.eval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'inspect_ai'"
     ]
    }
   ],
   "source": [
    "from inspect_ai.log import read_eval_log\n",
    "\n",
    "log = read_eval_log(log_file=r\"logs\\2025-03-26T23-43-33+01-00_nested-dirs-challenge_DxYcXZSDzKP9aKqDKAoYhq.eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check logs raw JSONs \n",
    "- change .eval to .zip \n",
    "- unzip \n",
    "- analyze headers.json and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\daniil anisimov\\.conda\\envs\\quist3\\lib\\site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\daniil anisimov\\.conda\\envs\\quist3\\lib\\site-packages (from seaborn) (2.2.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\daniil anisimov\\.conda\\envs\\quist3\\lib\\site-packages (from seaborn) (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\daniil anisimov\\.conda\\envs\\quist3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\daniil anisimov\\.conda\\envs\\quist3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\daniil anisimov\\.conda\\envs\\quist3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\daniil anisimov\\.conda\\envs\\quist3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\daniil anisimov\\.conda\\envs\\quist3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\daniil anisimov\\.conda\\envs\\quist3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\daniil anisimov\\.conda\\envs\\quist3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\daniil anisimov\\.conda\\envs\\quist3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\daniil anisimov\\.conda\\envs\\quist3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (6.4.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\daniil anisimov\\.conda\\envs\\quist3\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\daniil anisimov\\.conda\\envs\\quist3\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\daniil anisimov\\.conda\\envs\\quist3\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.4->seaborn) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\daniil anisimov\\.conda\\envs\\quist3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "def parse_component_scores(explanation_text):\n",
    "    \"\"\"\n",
    "    Parse component scores from explanation text using regex.\n",
    "    Looks for patterns like \"Depth: ✓\", \"Breadth: ✗\", etc.\n",
    "\n",
    "    Args:\n",
    "        explanation_text: The string containing the scorer explanation.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with binary scores for each component (0 or 1).\n",
    "        Defaults to 0 if a component is not found or parsing fails.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"depth_score\": 0,\n",
    "        \"breadth_score\": 0,\n",
    "        \"uniqueness_score\": 0\n",
    "    }\n",
    "\n",
    "    if not explanation_text:\n",
    "        return results\n",
    "\n",
    "    try:\n",
    "        # Search for \"Depth: [symbol]\" pattern\n",
    "        depth_match = re.search(r\"Depth:\\s*(✓|✗)\", explanation_text)\n",
    "        if depth_match:\n",
    "            results[\"depth_score\"] = 1 if depth_match.group(1) == \"✓\" else 0\n",
    "\n",
    "        # Search for \"Breadth: [symbol]\" pattern\n",
    "        breadth_match = re.search(r\"Breadth:\\s*(✓|✗)\", explanation_text)\n",
    "        if breadth_match:\n",
    "            results[\"breadth_score\"] = 1 if breadth_match.group(1) == \"✓\" else 0\n",
    "\n",
    "        # Search for \"Uniqueness: [symbol]\" pattern\n",
    "        uniqueness_match = re.search(r\"Uniqueness:\\s*(✓|✗)\", explanation_text)\n",
    "        if uniqueness_match:\n",
    "            results[\"uniqueness_score\"] = 1 if uniqueness_match.group(1) == \"✓\" else 0\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log the error but return the default scores\n",
    "        print(f\"Error parsing component scores with regex: {e}\")\n",
    "        # Ensure default values are returned on error\n",
    "        return {\n",
    "            \"depth_score\": 0,\n",
    "            \"breadth_score\": 0,\n",
    "            \"uniqueness_score\": 0\n",
    "        }\n",
    "\n",
    "# # --- Example Test ---\n",
    "# test_explanation_1 = \"[Py Direct /directory_task] Depth: ✓ (4/4), Breadth: ✓, Uniqueness: ✓Found 340...\"\n",
    "# test_explanation_2 = \"Depth: ✗, Breadth: ✓, Uniqueness: ✗\"\n",
    "# test_explanation_3 = \"Some other text, Breadth: ✓, Uniqueness: ✓ maybe depth is missing\"\n",
    "# test_explanation_4 = \"[Direct] Depth: ✓ (2/2), Breadth: ✓, Uniqueness: ✓ Found 6\" # From exp1 perhaps?\n",
    "\n",
    "# print(f\"Parsing: '{test_explanation_1[:50]}...' -> {parse_component_scores(test_explanation_1)}\")\n",
    "# print(f\"Parsing: '{test_explanation_2}' -> {parse_component_scores(test_explanation_2)}\")\n",
    "# print(f\"Parsing: '{test_explanation_3}' -> {parse_component_scores(test_explanation_3)}\")\n",
    "# print(f\"Parsing: '{test_explanation_4}' -> {parse_component_scores(test_explanation_4)}\")\n",
    "\n",
    "def extract_experiment_data(exp_dir: str, scorer_name: str = \"nested_dirs\"):\n",
    "    \"\"\"\n",
    "    Extract data from an experiment directory\n",
    "    \n",
    "    Args:\n",
    "        exp_dir: Path to the experiment directory\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with experiment metadata and results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    metadata = {}\n",
    "    print(f\"Processing {exp_dir}\")\n",
    "    # Load header.json for metadata\n",
    "    header_path = os.path.join(exp_dir, \"header.json\")\n",
    "    print(f\"Header path: {header_path}\")\n",
    "    if os.path.exists(header_path):\n",
    "        try:\n",
    "            with open(header_path, 'r', encoding='utf-8') as f:\n",
    "                header_data = json.load(f)\n",
    "                if \"eval\" in header_data:\n",
    "                    metadata = header_data[\"eval\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading header.json from {exp_dir}: {e}\")\n",
    "    \n",
    "    # Load summaries.json for results\n",
    "    summaries_path = os.path.join(exp_dir, \"summaries.json\")\n",
    "    print(f\"Summaries path: {summaries_path}\")\n",
    "    if os.path.exists(summaries_path):\n",
    "        try:\n",
    "            with open(summaries_path, 'r', encoding='utf-8') as f:\n",
    "                summaries_data = json.load(f)\n",
    "                \n",
    "                for sample in summaries_data:\n",
    "                    if \"scores\" in sample and scorer_name in sample[\"scores\"]:\n",
    "                        score_data = sample[\"scores\"][scorer_name]\n",
    "                        \n",
    "                        # Extract score value\n",
    "                        score_value = score_data.get(\"value\", 0)\n",
    "                        \n",
    "                        # Extract explanation and parse component scores\n",
    "                        explanation = score_data.get(\"explanation\", \"\")\n",
    "                        component_scores = parse_component_scores(explanation)\n",
    "                        \n",
    "                        # Create result entry\n",
    "                        result = {\n",
    "                            \"id\": sample.get(\"id\", \"\"),\n",
    "                            \"epoch\": sample.get(\"epoch\", 0),\n",
    "                            \"score_value\": score_value,\n",
    "                            \"explanation\": explanation,\n",
    "                            **component_scores\n",
    "                        }\n",
    "                        \n",
    "                        results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading summaries.json from {exp_dir}: {e}\")\n",
    "    \n",
    "    return {\n",
    "        \"metadata\": metadata,\n",
    "        \"results\": results\n",
    "    }\n",
    "\n",
    "def find_experiment_folders(base_dir):\n",
    "    \"\"\"\n",
    "    Find all experiment folders with 'nested-dirs-challenge' in the name\n",
    "    \n",
    "    Args:\n",
    "        base_dir: Base directory to search in\n",
    "        \n",
    "    Returns:\n",
    "        List of experiment directory paths\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(base_dir, \"*nested-dirs-challenge*\")\n",
    "    return glob.glob(pattern)\n",
    "\n",
    "def extract_all_experiments_data(base_dir):\n",
    "    \"\"\"\n",
    "    Extract data from all experiment folders\n",
    "    \n",
    "    Args:\n",
    "        base_dir: Base directory containing experiment folders\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with combined data from all experiments\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    # Find all experiment folders\n",
    "    exp_folders = find_experiment_folders(base_dir)\n",
    "    print(f\"Found {len(exp_folders)} experiment folders\")\n",
    "    \n",
    "    for folder in exp_folders:\n",
    "        try:\n",
    "            exp_data = extract_experiment_data(folder, scorer_name=\"check_nested_dirs_py\")\n",
    "            \n",
    "            # Extract model name from metadata\n",
    "            model_name = exp_data[\"metadata\"].get(\"model\", \"unknown\")\n",
    "            \n",
    "            for result in exp_data[\"results\"]:\n",
    "                result[\"model\"] = model_name\n",
    "                result[\"experiment_folder\"] = os.path.basename(folder)\n",
    "                \n",
    "                # Convert id to numerical value (task complexity)\n",
    "                # and potentially sample number\n",
    "                result_id_str = str(result[\"id\"]) # Ensure id is a string\n",
    "                result[\"sample_num\"] = None # Default sample number\n",
    "                result[\"task_complexity_val\"] = None # Default complexity value\n",
    "\n",
    "                try:\n",
    "                    # Try to interpret as a single integer complexity\n",
    "                    result[\"task_complexity_val\"] = int(result_id_str)\n",
    "                    result[\"task_complexity\"] = result[\"task_complexity_val\"] # Keep original field for now\n",
    "                except ValueError:\n",
    "                    # If not a single int, try to parse as \"sample_num, complexity_val\"\n",
    "                    if ',' in result_id_str:\n",
    "                        try:\n",
    "                            parts = result_id_str.split(',')\n",
    "                            if len(parts) == 2:\n",
    "                                sample_num_str = parts[0].strip()\n",
    "                                complexity_str = parts[1].strip()\n",
    "                                \n",
    "                                # Attempt to convert both parts to int\n",
    "                                sample_num = int(sample_num_str)\n",
    "                                complexity_val = int(complexity_str)\n",
    "                                \n",
    "                                result[\"sample_num\"] = sample_num\n",
    "                                result[\"task_complexity_val\"] = complexity_val\n",
    "                                # For compatibility or specific use, you might store the parsed string or individual parts\n",
    "                                result[\"task_complexity\"] = f\"Sample {sample_num}, Complexity {complexity_val}\"\n",
    "                            else:\n",
    "                                # If not two parts, keep original id\n",
    "                                result[\"task_complexity\"] = result_id_str\n",
    "                                print(f\"Warning: ID '{result_id_str}' contains a comma but not in 'sample, complexity' format. Storing as is.\")\n",
    "\n",
    "                        except ValueError:\n",
    "                            # If parts are not convertible to int, keep original id\n",
    "                            result[\"task_complexity\"] = result_id_str\n",
    "                            print(f\"Warning: Could not parse '{result_id_str}' as 'sample_num, complexity_val'. Storing as is.\")\n",
    "                    else:\n",
    "                        # If no comma and not an int, keep original id\n",
    "                        result[\"task_complexity\"] = result_id_str\n",
    "                        print(f\"Warning: ID '{result_id_str}' is not an integer and does not contain a comma. Storing as is.\")\n",
    "                \n",
    "                all_data.append(result)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing folder {folder}: {e}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    if all_data:\n",
    "        return pd.DataFrame(all_data)\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "def parse_component_scores(explanation_text):\n",
    "    \"\"\"\n",
    "    Parse component scores from explanation text using regex.\n",
    "    Looks for patterns like \"Depth: ✓\", \"Breadth: ✗\", etc.\n",
    "\n",
    "    Args:\n",
    "        explanation_text: The string containing the scorer explanation.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with binary scores for each component (0 or 1).\n",
    "        Defaults to 0 if a component is not found or parsing fails.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"depth_score\": 0,\n",
    "        \"breadth_score\": 0,\n",
    "        \"uniqueness_score\": 0\n",
    "    }\n",
    "\n",
    "    if not explanation_text:\n",
    "        # Return default scores if explanation_text is None or empty\n",
    "        return results\n",
    "    \n",
    "    if not isinstance(explanation_text, str):\n",
    "        # Ensure explanation_text is a string\n",
    "        print(f\"Warning: explanation_text is not a string: {type(explanation_text)}. Returning default scores.\")\n",
    "        return results\n",
    "\n",
    "    try:\n",
    "        # Search for \"Depth: [symbol]\" pattern\n",
    "        depth_match = re.search(r\"Depth:\\s*(✓|✗)\", explanation_text)\n",
    "        if depth_match:\n",
    "            results[\"depth_score\"] = 1 if depth_match.group(1) == \"✓\" else 0\n",
    "\n",
    "        # Search for \"Breadth: [symbol]\" pattern\n",
    "        breadth_match = re.search(r\"Breadth:\\s*(✓|✗)\", explanation_text)\n",
    "        if breadth_match:\n",
    "            results[\"breadth_score\"] = 1 if breadth_match.group(1) == \"✓\" else 0\n",
    "\n",
    "        # Search for \"Uniqueness: [symbol]\" pattern\n",
    "        uniqueness_match = re.search(r\"Uniqueness:\\s*(✓|✗)\", explanation_text)\n",
    "        if uniqueness_match:\n",
    "            results[\"uniqueness_score\"] = 1 if uniqueness_match.group(1) == \"✓\" else 0\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log the error but return the default scores\n",
    "        print(f\"Error parsing component scores with regex from '{explanation_text[:50]}...': {e}\")\n",
    "        # Ensure default values are returned on error\n",
    "        return {\n",
    "            \"depth_score\": 0,\n",
    "            \"breadth_score\": 0,\n",
    "            \"uniqueness_score\": 0\n",
    "        }\n",
    "\n",
    "def extract_experiment_data(exp_dir: str, scorer_name: str = \"nested_dirs\"):\n",
    "    \"\"\"\n",
    "    Extract data from an experiment directory.\n",
    "    Validates parameters and checks return values.\n",
    "    \n",
    "    Args:\n",
    "        exp_dir: Path to the experiment directory. Must be a non-empty string.\n",
    "        scorer_name: Name of the scorer. Must be a non-empty string.\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with experiment metadata and results.\n",
    "        Returns empty dicts if critical files are missing or errors occur.\n",
    "    \"\"\"\n",
    "    if not exp_dir or not isinstance(exp_dir, str):\n",
    "        print(\"Error: exp_dir must be a non-empty string.\")\n",
    "        return {\"metadata\": {}, \"results\": []}\n",
    "    if not scorer_name or not isinstance(scorer_name, str):\n",
    "        print(\"Error: scorer_name must be a non-empty string.\")\n",
    "        return {\"metadata\": {}, \"results\": []}\n",
    "\n",
    "    results_list = []\n",
    "    metadata = {}\n",
    "    print(f\"Processing {exp_dir}\")\n",
    "\n",
    "    # Load header.json for metadata\n",
    "    header_path = os.path.join(exp_dir, \"header.json\")\n",
    "    print(f\"Header path: {header_path}\")\n",
    "    if os.path.exists(header_path):\n",
    "        try:\n",
    "            with open(header_path, 'r', encoding='utf-8') as f:\n",
    "                header_data = json.load(f)\n",
    "                if \"eval\" in header_data and isinstance(header_data[\"eval\"], dict):\n",
    "                    metadata = header_data[\"eval\"]\n",
    "                else:\n",
    "                    print(f\"Warning: 'eval' key missing or not a dict in header.json from {exp_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading header.json from {exp_dir}: {e}\")\n",
    "            # metadata remains {}\n",
    "    else:\n",
    "        print(f\"Warning: header.json not found in {exp_dir}\")\n",
    "\n",
    "    # Load summaries.json for results\n",
    "    summaries_path = os.path.join(exp_dir, \"summaries.json\")\n",
    "    print(f\"Summaries path: {summaries_path}\")\n",
    "    if os.path.exists(summaries_path):\n",
    "        try:\n",
    "            with open(summaries_path, 'r', encoding='utf-8') as f:\n",
    "                summaries_data = json.load(f)\n",
    "                if not isinstance(summaries_data, list):\n",
    "                    print(f\"Error: summaries.json in {exp_dir} does not contain a list.\")\n",
    "                    summaries_data = [] # Process as empty list\n",
    "\n",
    "                for sample in summaries_data:\n",
    "                    if not isinstance(sample, dict):\n",
    "                        print(f\"Warning: Skipping non-dictionary item in summaries.json from {exp_dir}\")\n",
    "                        continue\n",
    "\n",
    "                    if \"scores\" in sample and isinstance(sample[\"scores\"], dict) and \\\n",
    "                       scorer_name in sample[\"scores\"] and isinstance(sample[\"scores\"][scorer_name], dict):\n",
    "                        \n",
    "                        score_data = sample[\"scores\"][scorer_name]\n",
    "                        \n",
    "                        score_value = score_data.get(\"value\", 0) # Default to 0 if missing\n",
    "                        if not isinstance(score_value, (int, float)):\n",
    "                            print(f\"Warning: Score value '{score_value}' is not numeric. Defaulting to 0 for sample ID {sample.get('id', 'N/A')}.\")\n",
    "                            score_value = 0\n",
    "\n",
    "                        explanation = score_data.get(\"explanation\", \"\") # Default to empty string\n",
    "                        component_scores_dict = parse_component_scores(explanation)\n",
    "                        \n",
    "                        result_entry = {\n",
    "                            \"id\": sample.get(\"id\", \"\"), # Default to empty string\n",
    "                            \"epoch\": sample.get(\"epoch\", 0), # Default to 0\n",
    "                            \"score_value\": score_value,\n",
    "                            \"explanation\": explanation,\n",
    "                            **component_scores_dict\n",
    "                        }\n",
    "                        results_list.append(result_entry)\n",
    "                    else:\n",
    "                        print(f\"Warning: Scorer '{scorer_name}' data missing or malformed for sample ID {sample.get('id', 'N/A')} in {exp_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or parsing summaries.json from {exp_dir}: {e}\")\n",
    "            # results_list remains as is (potentially partially filled or empty)\n",
    "    else:\n",
    "        print(f\"Warning: summaries.json not found in {exp_dir}\")\n",
    "    \n",
    "    return {\n",
    "        \"metadata\": metadata,\n",
    "        \"results\": results_list\n",
    "    }\n",
    "\n",
    "def find_experiment_folders(base_dir: str):\n",
    "    \"\"\"\n",
    "    Find all experiment folders with 'nested-dirs-challenge' in the name.\n",
    "    Validates parameters.\n",
    "    \n",
    "    Args:\n",
    "        base_dir: Base directory to search in. Must be a non-empty string.\n",
    "        \n",
    "    Returns:\n",
    "        List of experiment directory paths. Returns empty list on error.\n",
    "    \"\"\"\n",
    "    if not base_dir or not isinstance(base_dir, str):\n",
    "        print(\"Error: base_dir must be a non-empty string.\")\n",
    "        return []\n",
    "    \n",
    "    pattern = os.path.join(base_dir, \"*nested-dirs-challenge*\")\n",
    "    try:\n",
    "        folders = glob.glob(pattern)\n",
    "        return folders\n",
    "    except Exception as e:\n",
    "        print(f\"Error during glob.glob with pattern {pattern}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def extract_all_experiments_data(base_dir: str):\n",
    "    \"\"\"\n",
    "    Extract data from all experiment folders.\n",
    "    Validates parameters.\n",
    "    \n",
    "    Args:\n",
    "        base_dir: Base directory containing experiment folders. Must be a non-empty string.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with combined data from all experiments. Returns empty DataFrame on error or if no data.\n",
    "    \"\"\"\n",
    "    if not base_dir or not isinstance(base_dir, str):\n",
    "        print(\"Error: base_dir must be a non-empty string.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    all_data_list = []\n",
    "    \n",
    "    exp_folders = find_experiment_folders(base_dir)\n",
    "    if not exp_folders: # find_experiment_folders now returns list or handles its errors\n",
    "        print(f\"No experiment folders found in {base_dir} matching the pattern.\")\n",
    "        # Continue to return empty DataFrame if no folders\n",
    "    else:\n",
    "        print(f\"Found {len(exp_folders)} experiment folders\")\n",
    "    \n",
    "    for folder in exp_folders:\n",
    "        try:\n",
    "            # Specify the correct scorer name as used in your logs\n",
    "            exp_data_dict = extract_experiment_data(folder, scorer_name=\"check_nested_dirs_py\")\n",
    "            \n",
    "            # Validate return from extract_experiment_data\n",
    "            if not exp_data_dict or not isinstance(exp_data_dict, dict) or \\\n",
    "               \"metadata\" not in exp_data_dict or \"results\" not in exp_data_dict or \\\n",
    "               not isinstance(exp_data_dict[\"results\"], list):\n",
    "                print(f\"Warning: Invalid data structure returned from extract_experiment_data for folder {folder}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            model_name = exp_data_dict[\"metadata\"].get(\"model\", \"unknown\")\n",
    "            if not isinstance(model_name, str):\n",
    "                 print(f\"Warning: Model name is not a string: {model_name}. Using 'unknown_type'.\")\n",
    "                 model_name = \"unknown_type\"\n",
    "            \n",
    "            for result_item in exp_data_dict[\"results\"]:\n",
    "                if not isinstance(result_item, dict):\n",
    "                    print(f\"Warning: Skipping non-dictionary result item in folder {folder}.\")\n",
    "                    continue\n",
    "\n",
    "                result_item[\"model\"] = model_name\n",
    "                result_item[\"experiment_folder\"] = os.path.basename(folder)\n",
    "                \n",
    "                result_id_str = str(result_item.get(\"id\", \"\")) # Ensure id is string, default to empty\n",
    "                result_item[\"sample_num\"] = None \n",
    "                result_item[\"task_complexity_val\"] = None\n",
    "\n",
    "                try:\n",
    "                    result_item[\"task_complexity_val\"] = int(result_id_str)\n",
    "                    result_item[\"task_complexity\"] = result_item[\"task_complexity_val\"]\n",
    "                except ValueError:\n",
    "                    if ',' in result_id_str:\n",
    "                        try:\n",
    "                            parts = result_id_str.split(',')\n",
    "                            if len(parts) == 2:\n",
    "                                sample_num_str = parts[0].strip()\n",
    "                                complexity_str = parts[1].strip()\n",
    "                                \n",
    "                                sample_num = int(sample_num_str)\n",
    "                                complexity_val = int(complexity_str)\n",
    "                                \n",
    "                                result_item[\"sample_num\"] = sample_num\n",
    "                                result_item[\"task_complexity_val\"] = complexity_val\n",
    "                                result_item[\"task_complexity\"] = f\"Sample {sample_num}, Complexity {complexity_val}\"\n",
    "                            else:\n",
    "                                result_item[\"task_complexity\"] = result_id_str\n",
    "                                print(f\"Warning: ID '{result_id_str}' in {folder} contains a comma but not in 'sample, complexity' format. Storing as is.\")\n",
    "                        except ValueError:\n",
    "                            result_item[\"task_complexity\"] = result_id_str\n",
    "                            print(f\"Warning: Could not parse '{result_id_str}' in {folder} as 'sample_num, complexity_val'. Storing as is.\")\n",
    "                    else:\n",
    "                        result_item[\"task_complexity\"] = result_id_str\n",
    "                        if result_id_str: # Only print warning if id_str is not empty\n",
    "                            print(f\"Warning: ID '{result_id_str}' in {folder} is not an integer and does not contain a comma. Storing as is.\")\n",
    "                \n",
    "                all_data_list.append(result_item)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing folder {folder}: {e}\")\n",
    "            # Continue to next folder\n",
    "    \n",
    "    if all_data_list:\n",
    "        try:\n",
    "            df = pd.DataFrame(all_data_list)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating DataFrame from all_data_list: {e}\")\n",
    "            return pd.DataFrame() # Return empty DataFrame on error\n",
    "    else:\n",
    "        print(\"No data collected from any experiment folder.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def analyze_llm_evaluations(base_dir: str):\n",
    "    \"\"\"\n",
    "    Analyze LLM evaluation data, create visualizations, and save summary tables.\n",
    "    Validates parameters.\n",
    "    \n",
    "    Args:\n",
    "        base_dir: Base directory containing experiment folders. Must be a non-empty string.\n",
    "    \"\"\"\n",
    "    if not base_dir or not isinstance(base_dir, str):\n",
    "        print(\"Error: base_dir must be a non-empty string.\")\n",
    "        return\n",
    "\n",
    "    # Extract data from all experiments\n",
    "    df = extract_all_experiments_data(base_dir)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"No data found to analyze in the specified directories.\")\n",
    "        return\n",
    "    \n",
    "    # Define output directory\n",
    "    output_dir = os.path.join(base_dir, \"analysis\")\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    except OSError as e:\n",
    "        print(f\"Error creating output directory {output_dir}: {e}\")\n",
    "        return # Cannot proceed without output directory\n",
    "\n",
    "    # Display basic information\n",
    "    print(f\"\\nTotal records processed: {len(df)}\")\n",
    "    if 'model' in df.columns:\n",
    "        print(f\"Models evaluated: {df['model'].unique()}\")\n",
    "    else:\n",
    "        print(\"Warning: 'model' column not found in DataFrame.\")\n",
    "\n",
    "    if 'task_complexity_val' in df.columns:\n",
    "        df_numeric_complexity = df.dropna(subset=['task_complexity_val'])\n",
    "        if not df_numeric_complexity.empty and pd.api.types.is_numeric_dtype(df_numeric_complexity['task_complexity_val']):\n",
    "            min_comp = df_numeric_complexity['task_complexity_val'].min()\n",
    "            max_comp = df_numeric_complexity['task_complexity_val'].max()\n",
    "            print(f\"Task complexity value range: {min_comp} to {max_comp}\")\n",
    "        else:\n",
    "            print(\"No valid numeric task complexity values found for min/max range or column is not numeric.\")\n",
    "    else:\n",
    "        print(\"Warning: 'task_complexity_val' column not found for range calculation.\")\n",
    "    \n",
    "    # Extract and save amount of samples for each model\n",
    "    if 'model' in df.columns:\n",
    "        model_sample_counts = df.groupby('model').size().reset_index(name='number_of_samples')\n",
    "        print(\"\\nNumber of samples per model:\")\n",
    "        try:\n",
    "            print(model_sample_counts.to_string())\n",
    "            model_counts_csv_path = os.path.join(output_dir, \"model_sample_counts.csv\")\n",
    "            model_sample_counts.to_csv(model_counts_csv_path, index=False)\n",
    "            print(f\"\\nModel sample counts saved to {model_counts_csv_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing or saving model sample counts: {e}\")\n",
    "    else:\n",
    "        print(\"Skipping model sample counts as 'model' column is missing.\")\n",
    "\n",
    "    # Save the detailed raw data to CSV\n",
    "    # This llm_evaluation_data.csv contains all available columns from the DataFrame.\n",
    "    csv_path = os.path.join(output_dir, \"llm_evaluation_data.csv\")\n",
    "    try:\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"Detailed raw data saved to {csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving detailed raw data to CSV: {e}\")\n",
    "\n",
    "    # --- Create visualizations (only if necessary columns exist) ---\n",
    "    required_cols_for_plots = ['task_complexity_val', 'score_value', 'model', \n",
    "                               'depth_score', 'breadth_score', 'uniqueness_score']\n",
    "    missing_cols = [col for col in required_cols_for_plots if col not in df.columns]\n",
    "\n",
    "    if missing_cols:\n",
    "        print(f\"\\nWarning: Skipping visualizations due to missing columns: {', '.join(missing_cols)}\")\n",
    "        print(f\"Analysis (CSV exports) complete. Visualizations skipped. Output in {output_dir}\")\n",
    "        return\n",
    "\n",
    "    # Ensure 'task_complexity_val' is numeric for plotting\n",
    "    if not pd.api.types.is_numeric_dtype(df['task_complexity_val']):\n",
    "        print(\"\\nWarning: 'task_complexity_val' is not numeric. Skipping visualizations that require it as numeric.\")\n",
    "        print(f\"Analysis (CSV exports) complete. Visualizations skipped. Output in {output_dir}\")\n",
    "        return\n",
    "\n",
    "    component_columns = [\"depth_score\", \"breadth_score\", \"uniqueness_score\"]\n",
    "\n",
    "    try:\n",
    "        # 1. Overall score by model and task complexity value\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.boxplot(x=\"task_complexity_val\", y=\"score_value\", hue=\"model\", data=df)\n",
    "        plt.title(\"Overall Score by Model and Task Complexity Value\")\n",
    "        plt.xlabel(\"Task Complexity Value (N)\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.grid(linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, \"overall_score_by_model_complexity_value.png\"))\n",
    "        plt.close() # Close plot to free memory\n",
    "\n",
    "        # 2. Component scores by model (aggregated over complexity values for this plot)\n",
    "        # For this plot, we might want to average component scores per model, irrespective of task_complexity_val\n",
    "        # or show it per model and task_complexity_val if that's more insightful.\n",
    "        # Current component_long_df groups by model AND task_complexity_val which might be too granular for a simple bar plot by model.\n",
    "        # Let's adjust to average by model only for this specific bar plot:\n",
    "        component_avg_by_model_df = df.groupby(\"model\")[component_columns].mean().reset_index()\n",
    "        component_avg_by_model_long_df = pd.melt(\n",
    "            component_avg_by_model_df,\n",
    "            id_vars=[\"model\"],\n",
    "            value_vars=component_columns,\n",
    "            var_name=\"Component\",\n",
    "            value_name=\"Average Success Rate\"\n",
    "        )\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        sns.barplot(x=\"model\", y=\"Average Success Rate\", hue=\"Component\", data=component_avg_by_model_long_df)\n",
    "        plt.title(\"Average Component Success Rate by Model\")\n",
    "        plt.xlabel(\"Model\")\n",
    "        plt.ylabel(\"Average Success Rate (0-1)\")\n",
    "        plt.grid(linestyle='--', alpha=0.7)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, \"avg_component_scores_by_model.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # 3. Success rate heatmap by task complexity value and model\n",
    "        # Ensure task_complexity_val is suitable for pivot (e.g., not too many unique values)\n",
    "        if df['task_complexity_val'].nunique() < 20 : # Arbitrary threshold for heatmap readability\n",
    "            pivot_df = df.pivot_table(\n",
    "                index=\"model\", \n",
    "                columns=\"task_complexity_val\",\n",
    "                values=\"score_value\",\n",
    "                aggfunc=\"mean\"\n",
    "            )\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.heatmap(pivot_df, annot=True, cmap=\"YlGnBu\", vmin=0, vmax=1, fmt=\".2f\")\n",
    "            plt.title(\"Success Rate Heatmap by Model and Task Complexity Value\")\n",
    "            plt.xlabel(\"Task Complexity Value (N)\")\n",
    "            plt.ylabel(\"Model\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, \"success_rate_heatmap.png\"))\n",
    "            plt.close()\n",
    "        else:\n",
    "            print(\"Skipping heatmap generation as there are too many unique task_complexity_val values.\")\n",
    "\n",
    "        # 4. Performance by task complexity value for each model\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.lineplot(x=\"task_complexity_val\", y=\"score_value\", hue=\"model\", data=df, marker=\"o\", errorbar=('ci', 95))\n",
    "        plt.title(\"Model Performance by Task Complexity Value\")\n",
    "        plt.xlabel(\"Task Complexity Value (N)\")\n",
    "        plt.ylabel(\"Average Score\")\n",
    "        plt.grid(linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, \"performance_by_complexity_value.png\"))\n",
    "        plt.close()\n",
    "        \n",
    "        # 5. Component success rates by task complexity value\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True) # sharey might be useful\n",
    "        fig.suptitle(\"Component Success Rates by Task Complexity and Model\", fontsize=16) # Add a main title\n",
    "        \n",
    "        for i, component in enumerate(component_columns):\n",
    "            sns.lineplot(\n",
    "                x=\"task_complexity_val\", \n",
    "                y=component, \n",
    "                hue=\"model\", \n",
    "                data=df, \n",
    "                marker=\"o\",\n",
    "                ax=axes[i],\n",
    "                errorbar=('ci', 95)\n",
    "            )\n",
    "            axes[i].set_title(f\"{component.replace('_score', '').capitalize()} Success Rate\")\n",
    "            axes[i].set_xlabel(\"Task Complexity Value (N)\")\n",
    "            axes[i].set_ylabel(\"Success Rate\" if i == 0 else \"\") # Only show Y label on the first plot\n",
    "            axes[i].grid(linestyle='--', alpha=0.7)\n",
    "            axes[i].legend(title='Model') if i == len(component_columns) -1 else axes[i].legend().set_visible(False) # Show legend on last plot or adjust\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust layout to make space for suptitle\n",
    "        plt.savefig(os.path.join(output_dir, \"component_success_by_complexity_value.png\"))\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"\\nAnalysis and visualizations complete. Output saved to {output_dir}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during visualization generation: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 experiment folders\n",
      "Processing C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T15-32-20+02-00_nested-dirs-challenge_g9BVSLYE4CY5pQeMp8M6Gy\n",
      "Header path: C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T15-32-20+02-00_nested-dirs-challenge_g9BVSLYE4CY5pQeMp8M6Gy\\header.json\n",
      "Summaries path: C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T15-32-20+02-00_nested-dirs-challenge_g9BVSLYE4CY5pQeMp8M6Gy\\summaries.json\n",
      "Processing C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T15-32-20+02-00_nested-dirs-challenge_g9BVSLYE4CY5pQeMp8M6Gy.zip\n",
      "Header path: C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T15-32-20+02-00_nested-dirs-challenge_g9BVSLYE4CY5pQeMp8M6Gy.zip\\header.json\n",
      "Warning: header.json not found in C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T15-32-20+02-00_nested-dirs-challenge_g9BVSLYE4CY5pQeMp8M6Gy.zip\n",
      "Summaries path: C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T15-32-20+02-00_nested-dirs-challenge_g9BVSLYE4CY5pQeMp8M6Gy.zip\\summaries.json\n",
      "Warning: summaries.json not found in C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T15-32-20+02-00_nested-dirs-challenge_g9BVSLYE4CY5pQeMp8M6Gy.zip\n",
      "Processing C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-05-35+02-00_nested-dirs-challenge_ZzrRPqdtGdUjjdcF8cSzAV\n",
      "Header path: C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-05-35+02-00_nested-dirs-challenge_ZzrRPqdtGdUjjdcF8cSzAV\\header.json\n",
      "Summaries path: C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-05-35+02-00_nested-dirs-challenge_ZzrRPqdtGdUjjdcF8cSzAV\\summaries.json\n",
      "Processing C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-05-35+02-00_nested-dirs-challenge_ZzrRPqdtGdUjjdcF8cSzAV.zip\n",
      "Header path: C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-05-35+02-00_nested-dirs-challenge_ZzrRPqdtGdUjjdcF8cSzAV.zip\\header.json\n",
      "Warning: header.json not found in C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-05-35+02-00_nested-dirs-challenge_ZzrRPqdtGdUjjdcF8cSzAV.zip\n",
      "Summaries path: C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-05-35+02-00_nested-dirs-challenge_ZzrRPqdtGdUjjdcF8cSzAV.zip\\summaries.json\n",
      "Warning: summaries.json not found in C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-05-35+02-00_nested-dirs-challenge_ZzrRPqdtGdUjjdcF8cSzAV.zip\n",
      "Processing C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-36-30+02-00_nested-dirs-challenge_a9m2zF8qioTRmZVZ7iVb4G\n",
      "Header path: C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-36-30+02-00_nested-dirs-challenge_a9m2zF8qioTRmZVZ7iVb4G\\header.json\n",
      "Summaries path: C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-36-30+02-00_nested-dirs-challenge_a9m2zF8qioTRmZVZ7iVb4G\\summaries.json\n",
      "Warning: Scorer 'check_nested_dirs_py' data missing or malformed for sample ID 1, 9 in C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-36-30+02-00_nested-dirs-challenge_a9m2zF8qioTRmZVZ7iVb4G\n",
      "Warning: Scorer 'check_nested_dirs_py' data missing or malformed for sample ID 1, 3 in C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-36-30+02-00_nested-dirs-challenge_a9m2zF8qioTRmZVZ7iVb4G\n",
      "Warning: Scorer 'check_nested_dirs_py' data missing or malformed for sample ID 1, 3 in C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-36-30+02-00_nested-dirs-challenge_a9m2zF8qioTRmZVZ7iVb4G\n",
      "Warning: Scorer 'check_nested_dirs_py' data missing or malformed for sample ID 1, 7 in C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-36-30+02-00_nested-dirs-challenge_a9m2zF8qioTRmZVZ7iVb4G\n",
      "Warning: Scorer 'check_nested_dirs_py' data missing or malformed for sample ID 1, 3 in C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-36-30+02-00_nested-dirs-challenge_a9m2zF8qioTRmZVZ7iVb4G\n",
      "Processing C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-36-30+02-00_nested-dirs-challenge_a9m2zF8qioTRmZVZ7iVb4G.zip\n",
      "Header path: C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-36-30+02-00_nested-dirs-challenge_a9m2zF8qioTRmZVZ7iVb4G.zip\\header.json\n",
      "Warning: header.json not found in C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-36-30+02-00_nested-dirs-challenge_a9m2zF8qioTRmZVZ7iVb4G.zip\n",
      "Summaries path: C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-36-30+02-00_nested-dirs-challenge_a9m2zF8qioTRmZVZ7iVb4G.zip\\summaries.json\n",
      "Warning: summaries.json not found in C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-36-30+02-00_nested-dirs-challenge_a9m2zF8qioTRmZVZ7iVb4G.zip\n",
      "Processing C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-40-04+02-00_nested-dirs-challenge_TxYg5prczGXxBS9oYk4HPv\n",
      "Header path: C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-40-04+02-00_nested-dirs-challenge_TxYg5prczGXxBS9oYk4HPv\\header.json\n",
      "Summaries path: C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-40-04+02-00_nested-dirs-challenge_TxYg5prczGXxBS9oYk4HPv\\summaries.json\n",
      "Warning: Scorer 'check_nested_dirs_py' data missing or malformed for sample ID 1, 6 in C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-40-04+02-00_nested-dirs-challenge_TxYg5prczGXxBS9oYk4HPv\n",
      "Warning: Scorer 'check_nested_dirs_py' data missing or malformed for sample ID 1, 7 in C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-40-04+02-00_nested-dirs-challenge_TxYg5prczGXxBS9oYk4HPv\n",
      "Warning: Scorer 'check_nested_dirs_py' data missing or malformed for sample ID 1, 8 in C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-40-04+02-00_nested-dirs-challenge_TxYg5prczGXxBS9oYk4HPv\n",
      "Warning: Scorer 'check_nested_dirs_py' data missing or malformed for sample ID 1, 10 in C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-40-04+02-00_nested-dirs-challenge_TxYg5prczGXxBS9oYk4HPv\n",
      "Warning: Scorer 'check_nested_dirs_py' data missing or malformed for sample ID 1, 7 in C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-40-04+02-00_nested-dirs-challenge_TxYg5prczGXxBS9oYk4HPv\n",
      "Processing C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-40-04+02-00_nested-dirs-challenge_TxYg5prczGXxBS9oYk4HPv.zip\n",
      "Header path: C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-40-04+02-00_nested-dirs-challenge_TxYg5prczGXxBS9oYk4HPv.zip\\header.json\n",
      "Warning: header.json not found in C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-40-04+02-00_nested-dirs-challenge_TxYg5prczGXxBS9oYk4HPv.zip\n",
      "Summaries path: C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-40-04+02-00_nested-dirs-challenge_TxYg5prczGXxBS9oYk4HPv.zip\\summaries.json\n",
      "Warning: summaries.json not found in C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-40-04+02-00_nested-dirs-challenge_TxYg5prczGXxBS9oYk4HPv.zip\n",
      "Processing C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-46-56+02-00_nested-dirs-challenge_TFRyCNpW9F895A3ZC4b3v7\n",
      "Header path: C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-46-56+02-00_nested-dirs-challenge_TFRyCNpW9F895A3ZC4b3v7\\header.json\n",
      "Summaries path: C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-46-56+02-00_nested-dirs-challenge_TFRyCNpW9F895A3ZC4b3v7\\summaries.json\n",
      "Warning: Scorer 'check_nested_dirs_py' data missing or malformed for sample ID 1, 5 in C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-46-56+02-00_nested-dirs-challenge_TFRyCNpW9F895A3ZC4b3v7\n",
      "Warning: Scorer 'check_nested_dirs_py' data missing or malformed for sample ID 1, 7 in C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-46-56+02-00_nested-dirs-challenge_TFRyCNpW9F895A3ZC4b3v7\n",
      "Processing C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-46-56+02-00_nested-dirs-challenge_TFRyCNpW9F895A3ZC4b3v7.zip\n",
      "Header path: C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-46-56+02-00_nested-dirs-challenge_TFRyCNpW9F895A3ZC4b3v7.zip\\header.json\n",
      "Warning: header.json not found in C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-46-56+02-00_nested-dirs-challenge_TFRyCNpW9F895A3ZC4b3v7.zip\n",
      "Summaries path: C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-46-56+02-00_nested-dirs-challenge_TFRyCNpW9F895A3ZC4b3v7.zip\\summaries.json\n",
      "Warning: summaries.json not found in C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\2025-05-11T16-46-56+02-00_nested-dirs-challenge_TFRyCNpW9F895A3ZC4b3v7.zip\n",
      "\n",
      "Total records processed: 131\n",
      "Models evaluated: ['openai/gpt-4o-mini' 'openai/gpt-4o' 'google/gemini-1.5-flash'\n",
      " 'google/gemini-1.5-pro' 'google/gemini-2.5-pro-exp-03-25']\n",
      "Task complexity value range: 1 to 10\n",
      "\n",
      "Number of samples per model:\n",
      "                             model  number_of_samples\n",
      "0          google/gemini-1.5-flash                 12\n",
      "1            google/gemini-1.5-pro                 17\n",
      "2  google/gemini-2.5-pro-exp-03-25                 19\n",
      "3                    openai/gpt-4o                 38\n",
      "4               openai/gpt-4o-mini                 45\n",
      "\n",
      "Model sample counts saved to C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\analysis\\model_sample_counts.csv\n",
      "Detailed raw data saved to C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\analysis\\llm_evaluation_data.csv\n",
      "\n",
      "Analysis and visualizations complete. Output saved to C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\\analysis\n"
     ]
    }
   ],
   "source": [
    "base_dir = r\"C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp6_13.05\"\n",
    "    \n",
    "# Run the analysis\n",
    "analyze_llm_evaluations(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_all_experiments_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Debug - \u001b[39;00m\n\u001b[0;32m      2\u001b[0m base_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDaniil Anisimov\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mgit\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mspar_admission_evals\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mexp5_11.05\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m all_data_df \u001b[38;5;241m=\u001b[39m \u001b[43mextract_all_experiments_data\u001b[49m(base_dir)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(all_data_df)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'extract_all_experiments_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Debug - \n",
    "base_dir = r\"C:\\Users\\Daniil Anisimov\\git\\spar_admission_evals\\logs\\exp5_11.05\"\n",
    "all_data_df = extract_all_experiments_data(base_dir)\n",
    "print(all_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quist3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
